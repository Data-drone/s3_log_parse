{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "timely-dutch",
   "metadata": {},
   "source": [
    "## Testing Notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "tired-harassment",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = '/opt/spark-data'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "disciplinary-magic",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import os\n",
    "import sys\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import Row, StructField, StructType, StringType, IntegerType, ArrayType\n",
    "from pyspark.sql.functions import col, split, udf, size, element_at, explode, when, lit\n",
    "import pyspark.sql.functions as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "stupid-instruction",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"S3_Analysis\") \\\n",
    "    .master(\"spark://spark-master:7077\") \\\n",
    "    .config(\"spark.executor.cores\", \"2\") \\\n",
    "    .config(\"spark.num.executors\", \"6\") \\\n",
    "    .config(\"spark.executor.memory\", \"2g\") \\\n",
    "    .enableHiveSupport() \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "developing-librarian",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Open the parquet and have a look\n",
    "s3_stats = spark.read.parquet(os.path.join(path, \"s3logs\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "matched-story",
   "metadata": {},
   "outputs": [],
   "source": [
    "s3_stats.createOrReplaceTempView(\"s3_stats\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "electrical-conclusion",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(requesthour=15), Row(requesthour=14)]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"SELECT distinct requesthour from s3_stats\").collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "composite-coalition",
   "metadata": {},
   "outputs": [],
   "source": [
    "key_data = spark.sql(\"SELECT `key` FROM s3_stats\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "regular-salmon",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(min(requesttimestamp)=datetime.datetime(2020, 7, 14, 4, 0))]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check dates\n",
    "spark.sql(\"SELECT min(requesttimestamp) FROM s3_stats\").collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "rocky-sharp",
   "metadata": {},
   "source": [
    "We need to derive the tree structure from the key\n",
    "\n",
    "Scenarios:  \n",
    "- same name different parent  \n",
    "- same name same entity  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "capital-soviet",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Create some the parent child pairs we need to create out structure\n",
    "def zip_pairs(value):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        value (list(str)): split up list of folder path\n",
    "            ie [ db, table, partition, file.parquet ]\n",
    "    \n",
    "    Returns:\n",
    "        result (list((str, str)))\n",
    "    \n",
    "    \"\"\"\n",
    "    lead_list = value.copy()\n",
    "    lead_list.pop()\n",
    "    lead_list.insert(0,None)\n",
    "    result = [item for item in zip(lead_list,value)]\n",
    "  \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "extensive-watson",
   "metadata": {},
   "outputs": [],
   "source": [
    "expr = \".%25.\"\n",
    "pairZip = udf(zip_pairs, ArrayType(ArrayType(StringType())) )\n",
    "\n",
    "df2 = key_data.select(\"key\").withColumn(\"key_split\", split(col(\"key\"), \"/\")) \\\n",
    "        .withColumn(\"depth\", size(col(\"key_split\"))) \\\n",
    "        .withColumn(\"file\", element_at(col(\"key_split\"), -1) ) \\\n",
    "        .withColumn(\"pairs\", pairZip(col(\"key_split\"))) \\\n",
    "        .withColumn(\"_tmp\", explode(col(\"pairs\"))) \\\n",
    "        .withColumn(\"parent\", col(\"_tmp\")[0]) \\\n",
    "        .withColumn(\"object\", col(\"_tmp\")[1]) \\\n",
    "        .withColumn(\"_object_split\", split(col(\"object\"), \"\\.\")) \\\n",
    "        .withColumn(\"type\", when(size(\"_object_split\")>1, \"file\")\n",
    "                            .when(col(\"object\").rlike(expr), \"partition\")\n",
    "                            .otherwise(\"folder\")) \\\n",
    "        .drop(\"_tmp\") \\\n",
    "        .drop(\"_object_split\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "chronic-novelty",
   "metadata": {},
   "source": [
    "## Exploring Entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "urban-vinyl",
   "metadata": {},
   "outputs": [],
   "source": [
    "relationship_table = df2.select(\"parent\", \"object\", \"type\").distinct()\n",
    "relationship_table.createOrReplaceTempView(\"relationship_table\")\n",
    "#relationship_table.write.format(\"hive\").saveAsTable(\"logging_demo.relationship_table\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "intensive-happening",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1569470"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "relationship_table.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "institutional-opening",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(parent='part_key%253D2019276', count(*)=251),\n",
       " Row(parent='part_key%253D202076', count(*)=430),\n",
       " Row(parent='part_key%253D202044_new', count(*)=120),\n",
       " Row(parent='attempt_20200713230910_1265_m_000000_0', count(*)=0),\n",
       " Row(parent='onboard_file_version_av', count(*)=148),\n",
       " Row(parent='task_20200706104331_0011_m_000010', count(*)=0),\n",
       " Row(parent='task_20200714000305_1106_m_000000', count(*)=1),\n",
       " Row(parent='int_flat_timeusage', count(*)=10),\n",
       " Row(parent='attempt_20200714000716_0417_m_000000_0', count(*)=1),\n",
       " Row(parent='.hive-staging_hive_2020-07-04_17-25-51_057_8557720520923397094-428', count(*)=0)]"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"select parent, count(*) FILTER( WHERE type =='file' ) as `count(*)` from relationship_table group by parent\").head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fitting-airport",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Exploring just the files\n",
    "storage_files = df2.select(\"key\", \"parent\", \"child\", \"file\").filter( df2.child == df2.file )\n",
    "storage_files.filter(storage_files.parent.contains(\"=\")).show(40,truncate=False)\n",
    "\n",
    "## Exploring the folders in the tree\n",
    "folders = df2.select(\"key\", \"parent\", \"child\", \"file\").filter( df2.child != df2.file )\n",
    "folders.select(\"parent\", \"child\").show(10, truncate=False)\n",
    "\n",
    "distinct_folders = folders.select(\"child\").distinct()\n",
    "\n",
    "entities = spark.sql(\"SELECT parent as stage from relationship_table \\\n",
    "                     UNION SELECT child as stage from relationship_table\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "illegal-brazilian",
   "metadata": {},
   "source": [
    "## End"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "standing-preparation",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:spark]",
   "language": "python",
   "name": "conda-env-spark-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
